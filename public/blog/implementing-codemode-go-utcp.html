<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Implementing CodeMode in go-utcp | Kamil Mościszko</title>
    <meta name="description"
        content="Why LLMs are better at writing code than calling tools - and how CodeMode UTCP leverages this to build more capable AI agents." />

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;500;600;700;800&family=Space+Grotesk:wght@300;400;500;600&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="/blog-style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css">
</head>

<body>
    <nav>
        <a href="/index.html" class="logo">← Back to Portfolio</a>
        <div class="links">
            <a href="/index.html#about">About</a>
            <a href="/index.html#projects">Projects</a>
            <a href="/index.html#blog">Blog</a>
        </div>
    </nav>

    <main class="blog-post">
        <article>
            <header class="post-header">
                <div class="post-meta">
                    <span class="post-date">November 20, 2025</span>
                    <span class="post-read-time">15 min read</span>
                </div>
                <h1>Code Mode: Why LLMs Should Write Code, Not Call Tools</h1>
                <div class="post-tags">
                    <span class="tag">Go</span>
                    <span class="tag">AI</span>
                    <span class="tag">UTCP</span>
                    <span class="tag">LLM</span>
                    <span class="tag">Architecture</span>
                </div>
            </header>

            <div class="post-content">
                <h2>The Problem with Tool Calling</h2>
                <p>It turns out we've all been using tool-calling protocols wrong. Most AI agents today expose tools
                    directly to LLMs using special tokens and JSON schemas. But there's a better way: <strong>let the
                        LLM write code that calls an API</strong>.</p>

                <p>This article explores <strong>CodeMode UTCP</strong> - an implementation of this approach for the
                    <a href="https://github.com/universal-tool-calling-protocol/go-utcp" target="_blank">Universal Tool
                        Calling Protocol</a> that uses the <a href="https://github.com/traefik/yaegi"
                        target="_blank">Yaegi Go interpreter</a> to safely execute LLM-generated code.
                </p>

                <hr>

                <h2>What's Wrong with Traditional Tool Calling?</h2>

                <p>Under the hood, when an LLM uses a "tool," it generates special tokens that don't represent actual
                    text. The LLM is trained to output these tokens in a specific format:</p>

                <pre><code class="language-text">I will check the weather in Austin, TX.
&lt;|tool_call|&gt; 
{ 
  "name": "get_weather", 
  "arguments": { "location": "Austin, TX" } 
} 
&lt;|end_tool_call|&gt;</code></pre>

                <p>The harness intercepts these special tokens, parses the JSON, calls the tool, and feeds the result
                    back:</p>

                <pre><code class="language-text">&lt;|tool_result|&gt; 
{ 
  "temperature": 93, 
  "conditions": "sunny" 
} 
&lt;|end_tool_result|&gt;</code></pre>

                <h3>The Core Issue</h3>

                <p>These special tokens are things <strong>LLMs have never seen in the wild</strong>. They must be
                    specially trained using synthetic data. As a result:</p>

                <ul>
                    <li><strong>Limited capability</strong> - LLMs struggle with complex tools or too many options</li>
                    <li><strong>Simplified APIs</strong> - Tool designers must "dumb down" their interfaces</li>
                    <li><strong>Poor reliability</strong> - The LLM may choose the wrong tool or use it incorrectly
                    </li>
                </ul>

                <p>Meanwhile, LLMs are <em>excellent</em> at writing code. They've seen millions of real-world code
                    examples from open source projects. <strong>Writing code and calling tools are almost the same
                        thing</strong> - so why can LLMs do one much better than the other?</p>

                <blockquote>
                    <p>Making an LLM perform tasks with tool calling is like putting Shakespeare through a month-long
                        class in Mandarin and then asking him to write a play in it. It's just not going to be his best
                        work.</p>
                </blockquote>

                <hr>

                <h2>The Code Mode Solution</h2>

                <p>Instead of asking the LLM to use special tokens, we:</p>

                <ol>
                    <li><strong>Convert tools into a code API</strong> - Generate type-safe function signatures from
                        tool schemas</li>
                    <li><strong>Ask the LLM to write code</strong> - Let it use its natural code-writing abilities</li>
                    <li><strong>Execute in a sandbox</strong> - Run the generated code safely with access only to the
                        tool API</li>
                </ol>

                <p>The results are striking. LLMs can now:</p>
                <ul>
                    <li>Handle complex, multi-step workflows naturally</li>
                    <li>Use full-featured APIs without simplification</li>
                    <li>Implement proper error handling and retry logic</li>
                    <li>Chain tool calls with conditional logic</li>
                </ul>

                <hr>

                <h2>CodeMode UTCP Architecture</h2>

                <p>Our Go implementation consists of two main components:</p>

                <h3>1. The Orchestrator (<code>orchestrator.go</code>)</h3>
                <p>An LLM-driven pipeline that:</p>
                <ul>
                    <li>Determines if tools are needed for a query</li>
                    <li>Selects relevant tools from available UTCP tools</li>
                    <li>Generates Go code using only selected tools</li>
                    <li>Validates generated code against strict rules</li>
                </ul>

                <h3>2. The Execution Engine (<code>codemode.go</code>)</h3>
                <p>A sandboxed runtime that:</p>
                <ul>
                    <li>Uses Yaegi Go interpreter for safe execution</li>
                    <li>Injects helper functions for UTCP tool access</li>
                    <li>Normalizes LLM output into valid Go programs</li>
                    <li>Enforces timeouts and captures output</li>
                </ul>

                <hr>

                <h2>The Four-Step Pipeline</h2>

                <p>When a user makes a request, CodeMode UTCP executes four distinct steps:</p>

                <h3>Step 1: Decide if Tools Are Needed</h3>

                <p>First, we ask the LLM whether the query requires external tools at all:</p>

                <pre><code class="language-go">func (cm *CodeModeUTCP) decideIfToolsNeeded(
    ctx context.Context,
    query string,
    tools string,
) (bool, error) {
    prompt := fmt.Sprintf(`
Decide if the following user query requires using ANY UTCP tools.

USER QUERY: %q
AVAILABLE UTCP TOOLS: %s

Respond ONLY in JSON: { "needs": true } or { "needs": false }
`, query, tools)
    
    raw, err := cm.model.Generate(ctx, prompt)
    // ... parse JSON response
}</code></pre>

                <p>This prevents unnecessary tool calls for simple queries like "What is 2+2?"</p>

                <h3>Step 2: Select Relevant Tools</h3>

                <p>Next, we identify which specific tools are needed:</p>

                <pre><code class="language-go">func (cm *CodeModeUTCP) selectTools(
    ctx context.Context,
    query string,
    tools string,
) ([]string, error) {
    prompt := fmt.Sprintf(`
Select ALL UTCP tools that match the user's intent.

USER QUERY: %q
AVAILABLE UTCP TOOLS: %s

Respond ONLY in JSON:
{
  "tools": ["provider.tool", ...]
}

Rules:
- Use ONLY names listed above
- NO modifications, NO guessing
- If multiple tools apply, include all
`, query, tools)
    
    raw, err := cm.model.Generate(ctx, prompt)
    // Returns: ["math.add", "math.multiply"]
}</code></pre>

                <p>This narrows the context to only relevant tools, improving code generation quality.</p>

                <h3>Step 3: Generate Go Code</h3>

                <p>Now comes the magic - the LLM writes actual Go code:</p>

                <pre><code class="language-go">// Example generated code for: "Get sum of 5 and 7, then multiply by 3"

r1, err := codemode.CallTool("math.add", map[string]any{
    "a": 5,
    "b": 7,
})
if err != nil { return err }

var sum any
if m, ok := r1.(map[string]any); ok {
    sum = m["result"]
}

r2, err := codemode.CallTool("math.multiply", map[string]any{
    "a": sum,
    "b": 3,
})

__out = map[string]any{
    "sum": sum,
    "product": r2,
}</code></pre>

                <p><strong>Key constraints enforced:</strong></p>
                <ul>
                    <li>Use only selected tool names (no inventing tools)</li>
                    <li>Use exact input/output schema keys from tool specs</li>
                    <li>No package/import declarations (added automatically)</li>
                    <li>Assign final result to <code>__out</code> variable</li>
                    <li>Use only provided helper functions</li>
                </ul>

                <h3>Step 4: Execute in Sandbox</h3>

                <p>Finally, we execute the code safely:</p>

                <pre><code class="language-go">func (c *CodeModeUTCP) Execute(
    ctx context.Context,
    args CodeModeArgs,
) (CodeModeResult, error) {
    // Enforce timeout
    ctx, cancel := context.WithTimeout(ctx, 
        time.Duration(args.Timeout)*time.Millisecond)
    defer cancel()
    
    i, stdout, stderr := newInterpreter()
    
    // Inject UTCP helpers
    injectHelpers(i, c.client)
    
    // Wrap and prepare code
    wrapped := c.prepareWrappedProgram(args.Code)
    
    // Run in goroutine with panic recovery
    done := make(chan evalResult, 1)
    go func() {
        defer func() {
            if r := recover(); r != nil {
                done <- evalResult{err: fmt.Errorf("panic: %v", r)}
            }
        }()
        
        v, err := i.Eval(wrapped)
        done <- evalResult{val: v, err: err}
    }()
    
    // Wait for completion or timeout
    select {
    case <-ctx.Done():
        return CodeModeResult{}, fmt.Errorf("timeout")
    case res := <-done:
        return CodeModeResult{
            Value: res.val.Interface(),
            Stdout: stdout.String(),
            Stderr: stderr.String(),
        }, res.err
    }
}</code></pre>

                <hr>

                <h2>Making LLM Output Executable</h2>

                <p>LLMs don't always generate perfect Go code. We apply several normalization steps:</p>

                <h3>1. Package/Import Stripping</h3>
                <p>LLMs often include <code>package main</code> or <code>import</code> statements. We strip these since
                    the wrapper adds them automatically.</p>

                <h3>2. Walrus Operator Conversion</h3>
                <pre><code class="language-go">func convertOutWalrus(code string) string {
    // Converts: __out := ... 
    // To:       __out = ...
    re := regexp.MustCompile(`__out\s*:=`)
    return re.ReplaceAllString(code, "__out = ")
}</code></pre>

                <p>The <code>__out</code> variable is pre-declared, so <code>:=</code> would cause a redeclaration
                    error.</p>

                <h3>3. JSON to Go Literal Conversion</h3>
                <p>LLMs sometimes output JSON objects instead of Go map literals:</p>

                <pre><code class="language-go">func toGoLiteral(v any) string {
    switch val := v.(type) {
    case map[string]any:
        parts := make([]string, 0, len(val))
        for k, v2 := range val {
            parts = append(parts, 
                fmt.Sprintf("%q: %s", k, toGoLiteral(v2)))
        }
        return fmt.Sprintf("map[string]any{%s,}", 
            strings.Join(parts, ", "))
    case []any:
        items := make([]string, len(val))
        for i := range val {
            items[i] = toGoLiteral(val[i])
        }
        return fmt.Sprintf("[]any{%s}", strings.Join(items, ", "))
    case string:
        return fmt.Sprintf("%q", val)
    // ... other cases
    }
}</code></pre>

                <hr>

                <h2>Injecting the Tool API</h2>

                <p>The sandboxed environment needs access to UTCP tools. We inject helper functions using Yaegi's
                    reflection-based exports:</p>

                <pre><code class="language-go">func injectHelpers(i *interp.Interpreter, client utcp.UtcpClientInterface) error {
    i.Use(stdlib.Symbols) // Load Go standard library
    
    exports := interp.Exports{
        "codemode_helpers/codemode_helpers": map[string]reflect.Value{
            "CallTool": reflect.ValueOf(func(name string, args map[string]any) (any, error) {
                return client.CallTool(context.Background(), name, args)
            }),
            
            "CallToolStream": reflect.ValueOf(func(name string, args map[string]any) (*codeModeStream, error) {
                stream, err := client.CallToolStream(context.Background(), name, args)
                if err != nil {
                    return nil, err
                }
                return &codeModeStream{next: stream.Next}, nil
            }),
            
            "SearchTools": reflect.ValueOf(func(query string, limit int) ([]tools.Tool, error) {
                return client.SearchTools(query, limit)
            }),
            
            "Sprintf": reflect.ValueOf(fmt.Sprintf),
            "Errorf": reflect.ValueOf(fmt.Errorf),
        },
    }
    
    return i.Use(exports)
}</code></pre>

                <p>These functions become available in generated code as <code>codemode.CallTool()</code>,
                    <code>codemode.Sprintf()</code>, etc.
                </p>

                <hr>

                <h2>Security: Sandboxing LLM-Generated Code</h2>

                <p>Running code written by an LLM requires careful isolation:</p>

                <h3>1. Yaegi Interpreter</h3>
                <ul>
                    <li><strong>No filesystem access</strong> - Unless explicitly provided via helpers</li>
                    <li><strong>No network access</strong> - Isolated from the Internet</li>
                    <li><strong>No process spawning</strong> - Cannot execute external commands</li>
                    <li><strong>Isolated namespace</strong> - Runs in same process but separate context</li>
                </ul>

                <h3>2. Timeout Enforcement</h3>
                <p>Default 30-second timeout prevents infinite loops and runaway execution.</p>

                <h3>3. Panic Recovery</h3>
                <p>Interpreter panics are caught and returned as structured errors, preventing crashes.</p>

                <h3>4. Validation Rules</h3>
                <p>Basic syntax validation ensures code contains required elements before execution.</p>

                <hr>

                <h2>Real-World Example</h2>

                <p>Let's walk through a complete workflow:</p>

                <p><strong>User Query:</strong> "Search for Python tutorials and summarize the top 3 results"</p>

                <h3>Step 1: Decide Tools Needed</h3>
                <p>→ <strong>Result:</strong> <code>{ "needs": true }</code></p>

                <h3>Step 2: Select Tools</h3>
                <p>→ <strong>Result:</strong> <code>{ "tools": ["search.web", "text.summarize"] }</code></p>

                <h3>Step 3: Generate Code</h3>
                <pre><code class="language-go">// Search for tutorials
searchResult, err := codemode.CallTool("search.web", map[string]any{
    "query": "Python tutorials",
    "limit": 3,
})
if err != nil { return err }

var results []any
if m, ok := searchResult.(map[string]any); ok {
    if r, ok := m["results"].([]any); ok {
        results = r
    }
}

// Extract top 3 and summarize
var summaries []any
for i := 0; i < 3 && i < len(results); i++ {
    item := results[i]
    
    var text string
    if m, ok := item.(map[string]any); ok {
        if t, ok := m["content"].(string); ok {
            text = t
        }
    }
    
    summary, err := codemode.CallTool("text.summarize", map[string]any{
        "text": text,
        "max_length": 100,
    })
    if err != nil { continue }
    
    summaries = append(summaries, summary)
}

__out = map[string]any{
    "query": "Python tutorials",
    "summaries": summaries,
}</code></pre>

                <h3>Step 4: Execute</h3>
                <p>The code runs in the Yaegi sandbox, calls the actual UTCP tools, and returns structured results—all
                    in one execution.</p>

                <hr>

                <h2>Why This Works Better</h2>

                <p>CodeMode UTCP demonstrates that <strong>code is a better interface for tool orchestration than
                        special tokens</strong>. By leveraging LLMs' natural code-writing abilities, we achieve:</p>

                <ul>
                    <li>✅ <strong>Lower latency</strong> - One execution vs. multiple round-trips</li>
                    <li>✅ <strong>Better composability</strong> - Express complex workflows naturally</li>
                    <li>✅ <strong>Robust error handling</strong> - Standard Go error patterns</li>
                    <li>✅ <strong>Full API access</strong> - No need to simplify tool interfaces</li>
                    <li>✅ <strong>Higher reliability</strong> - LLMs excel at writing code</li>
                    <li>✅ <strong>Debuggability</strong> - Inspect generated code and execution logs</li>
                </ul>

                <p>The key insight: <strong>LLMs have seen millions of code examples but only synthetic tool-calling
                        data</strong>. We should play to their strengths.</p>

                <hr>

                <h2>Performance Characteristics</h2>

                <h3>Latency Breakdown</h3>
                <ul>
                    <li><strong>LLM calls:</strong> 3 sequential calls (decide, select, generate) ≈ 2-5 seconds</li>
                    <li><strong>Code execution:</strong> Typically <100ms for simple workflows</li>
                    <li><strong>Tool calls:</strong> Depends on tool implementation</li>
                </ul>

                <h3>Optimization Opportunities</h3>
                <ul>
                    <li><strong>Parallel LLM calls</strong> - Decide + Select could run concurrently</li>
                    <li><strong>Caching</strong> - Cache tool specs to reduce prompt size</li>
                    <li><strong>Streaming</strong> - Stream code generation for faster perceived latency</li>
                    <li><strong>Compiled mode</strong> - Pre-compile common patterns</li>
                </ul>

                <hr>

                <h2>Limitations and Trade-offs</h2>

                <h3>Current Limitations</h3>
                <ul>
                    <li><strong>Single-threaded execution</strong> - Yaegi runs code sequentially</li>
                    <li><strong>No filesystem access</strong> - Unless explicitly provided via helpers</li>
                    <li><strong>LLM quality dependency</strong> - Bad code generation = runtime errors</li>
                    <li><strong>Debugging complexity</strong> - Stack traces from interpreted code can be cryptic</li>
                </ul>

                <h3>Design Trade-offs</h3>
                <ul>
                    <li><strong>Safety vs. Flexibility</strong> - Sandboxing limits what code can do</li>
                    <li><strong>Simplicity vs. Power</strong> - Go DSL is more constrained than Python</li>
                    <li><strong>Latency vs. Reliability</strong> - Multiple LLM calls increase latency but improve
                        correctness</li>
                </ul>

                <hr>

                <h2>Try It Yourself</h2>

                <p>CodeMode UTCP is part of the <a href="https://github.com/universal-tool-calling-protocol/go-utcp"
                        target="_blank">go-utcp</a> repository:</p>

                <pre><code class="language-bash">git clone https://github.com/universal-tool-calling-protocol/go-utcp
cd go-utcp/src/plugins/codemode
go test -v</code></pre>

                <p>Check out the <a
                        href="https://github.com/universal-tool-calling-protocol/go-utcp/tree/main/src/plugins/codemode"
                        target="_blank">README</a> for more examples and API documentation.</p>

                <hr>

                <h2>Related Work</h2>

                <p>CodeMode UTCP builds on ideas from:</p>
                <ul>
                    <li><a href="https://blog.cloudflare.com/code-mode/" target="_blank">Cloudflare's Code Mode</a> -
                        TypeScript-based approach for MCP</li>
                    <li><a href="https://github.com/traefik/yaegi" target="_blank">Yaegi</a> - Go interpreter</li>
                    <li><a href="https://github.com/universal-tool-calling-protocol" target="_blank">UTCP</a> -
                        Universal Tool Calling Protocol</li>
                    <li><a href="https://github.com/Protocol-Lattice" target="_blank">Protocol Lattice</a> - AI
                        infrastructure ecosystem</li>
                </ul>

                <p>Special thanks to the UTCP community for feedback and contributions.</p>

                <hr>

                <p class="post-footer-note"><em>Have questions or ideas? Reach out on <a
                            href="https://github.com/universal-tool-calling-protocol/go-utcp/issues"
                            target="_blank">GitHub</a> or <a href="mailto:kmosc@protonmail.com">email me</a>.</em></p>
            </div>
        </article>
    </main>

    <footer>
        <p>&copy; 2025 Kamil Mościszko. <a href="/index.html">Back to Portfolio</a></p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
</body>

</html>